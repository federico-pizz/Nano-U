# Nano-U Project - Master Implementation Plan
## Complete Simplification & NAS Redesign

**Date**: 2026-01-16  
**Status**: Ready for Implementation  
**Goal**: Simplify overcomplicated project, fix bugs, implement callback-based NAS monitoring

---

## Table of Contents
1. [Executive Summary](#executive-summary)
2. [Critical Issues Found](#critical-issues-found)
3. [Architecture Decisions](#architecture-decisions)
4. [NAS Simplification Strategy](#nas-simplification-strategy)
5. [Complete Implementation Plan](#complete-implementation-plan)
6. [Visualization & Metrics](#visualization--metrics)
7. [Testing & Validation](#testing--validation)
8. [Success Criteria](#success-criteria)

---

## Executive Summary

### Project Goal
Nano-U is a tiny segmentation model for ESP32-S3 deployment. The project evolved from a working PyTorch implementation but became overcomplicated during the TensorFlow port with architectural mismatches and broken NAS integration.

### What We've Completed
1. ‚úÖ **Nano_U Architecture**: Removed skip connections to match original simple autoencoder **[COMPLETED]**
2. ‚úÖ **NAS Integration**: Replaced broken ActivationExtractor with callback-based monitoring **[COMPLETED]**
3. ‚úÖ **Code Simplification**: Removed complex nested model workarounds **[COMPLETED]**
4. ‚úÖ **Visualization**: Added comprehensive metrics plotting (4 plot types) **[COMPLETED]**
5. ‚úÖ **Test Suite**: 13/13 tests passing for NAS callback **[COMPLETED]**
6. ‚úÖ **Bug Fixes**: Fixed 5 critical bugs in training and plotting **[COMPLETED]**

### Critical Issue Remaining
‚ö†Ô∏è **NAS Monitoring Returns Zero Metrics**: The callback monitors model OUTPUT (single channel) instead of internal CONV layers. For meaningful analysis, need to add layer selector support to monitor internal activations.

### Key Decisions Made
- **Architecture Philosophy**: Nano_U = Simple autoencoder (NO skip connections)
- **NAS Approach**: Callback-based monitoring (non-invasive, compatible with any model)
- **Priority**: Training works, NAS integrated but needs enhancement for internal layer monitoring

---

## Critical Issues - Status Update

### Issue #1: Nano_U Had Wrong Architecture ‚úÖ FIXED
**Problem**: Had U-Net skip connections, but old working version didn't have them

**Fix Applied**: Removed skip connections from [`src/models/Nano_U/model_tf.py:84-110`](../src/models/Nano_U/model_tf.py)

**Result**: Architecture now matches original simple autoencoder

---

### Issue #2: NAS Layer Detection Was Broken ‚úÖ PARTIALLY FIXED
**Problem**: ActivationExtractor couldn't find layers in nested subclassed models

**Fix Applied**: Replaced with callback-based monitoring (doesn't need layer introspection)

**Current Status**: ‚úÖ Callback works, but ‚ö†Ô∏è monitors OUTPUT instead of internal layers

**Remaining Problem**: For single-output segmentation (1 channel), covariance analysis yields:
- `trace = 0` (sum of eigenvalues for 1√ó1 matrix)
- `condition_number = 1` (only one eigenvalue)
- `redundancy_score = 0` (no off-diagonal elements)

**Solution Needed**: Modify `NASMonitorCallback` to accept layer selectors and use `ActivationExtractor` internally:

```python
# Proposed fix:
nas_callback = NASMonitorCallback(
    validation_data=val_ds,
    layer_selectors=["encoder_conv1", "encoder_conv2", "bottleneck"],  # NEW
    log_dir="logs/nas"
)
```

**Priority**: HIGH - blocks useful NAS analysis

---

### Issue #3: Overcomplicated Workarounds ‚úÖ FIXED
**Problem**: `inner_model` parameter passed around trying to fix issue #2

**Fix Applied**: Removed all workarounds, simplified `train_with_nas.py` to delegate to `train.py`

**Result**: Clean, maintainable code

---

## NEW ISSUES DISCOVERED & FIXED

### Bug #4: Directory Creation Failure ‚úÖ FIXED
**File**: [`src/train.py:311-313`](../src/train.py)
**Problem**: `os.makedirs(os.path.dirname('file.csv'))` fails when dirname returns empty string

**Fix**:
```python
csv_dir = os.path.dirname(nas_csv_path)
if csv_dir:  # Only create if there's a directory part
    os.makedirs(csv_dir, exist_ok=True)
```

---

### Bug #5: NASMonitorCallback Parameter Mismatch ‚úÖ FIXED
**File**: [`src/train.py:315-321`](../src/train.py)
**Problem**: Wrong parameter names when initializing callback

**Fix**: Changed `log_freq` ‚Üí `monitor_frequency`, `monitor_batch_freq` ‚Üí `log_frequency`, added `validation_data` parameter

---

### Bug #6: Missing validation_data in Callback ‚úÖ FIXED
**File**: [`src/nas_covariance.py:619`](../src/nas_covariance.py)
**Problem**: Keras doesn't automatically pass `validation_data` to callbacks

**Fix**: Modified `NASMonitorCallback.__init__()` to accept `validation_data` as explicit parameter

---

### Bug #7: Seaborn Import Error ‚úÖ FIXED
**File**: [`src/plot_nas_metrics.py:32-37`](../src/plot_nas_metrics.py)
**Problem**: Hard requirement on seaborn which isn't installed

**Fix**: Made seaborn optional with try/except import

---

### Bug #8: Column Name Mismatch in Plots ‚úÖ FIXED
**File**: [`src/plot_nas_metrics.py:45-66`](../src/plot_nas_metrics.py)
**Problem**: Code expected `correlation_mean` but CSV saved `mean_correlation`

**Fix**: Added normalization logic in `load_nas_metrics()` to handle both column names

---

## Architecture Decisions

### Nano_U: Simple Autoencoder (NOT U-Net)

#### Old Implementation (Working) ‚úÖ
```
Input (48x64x3)
    ‚Üì
[DoubleConv 32] ‚îÄ‚Üí (pool) ‚îÄ‚Üí [DoubleConv 64] ‚îÄ‚Üí (pool) ‚îÄ‚Üí [DoubleConv 128]
                                                                    ‚Üì
                                                            [Bottleneck 128]
                                                                    ‚Üì
[DoubleConv 32] ‚Üê‚îÄ (upsample) ‚Üê‚îÄ [DoubleConv 64] ‚Üê‚îÄ (upsample) ‚Üê‚îÄ
    ‚Üì
[Output Conv 1x1] ‚Üí (48x64x1)

Parameters: ~41K
Skip Connections: NONE
```

#### New Implementation (Current - WRONG) ‚ùå
```
Input (48x64x3)
    ‚Üì
[DepthSepConv 16] ‚îÄ‚îÄ‚îê
    ‚Üì pool          ‚îÇ skip1
[DepthSepConv 32] ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îê
    ‚Üì pool          ‚îÇ  ‚îÇ skip2
[DepthSepConv 64]   ‚îÇ  ‚îÇ
    ‚Üì               ‚îÇ  ‚îÇ
[Bottleneck 64]     ‚îÇ  ‚îÇ
    ‚Üì upsample      ‚îÇ  ‚îÇ
[Concat + Conv 32] ‚Üê‚îò  ‚îÇ
    ‚Üì upsample         ‚îÇ
[Concat + Conv 16] ‚Üê‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
[Output Conv 1x1]

Parameters: ~19K (with skips)
Skip Connections: YES (U-Net style)
```

**Decision**: Remove skip connections to match old simple autoencoder

---

## NAS Simplification Strategy

### Previous Approach (Broken)
```
ActivationExtractor ‚Üí Find intermediate layers ‚Üí Monitor covariance ‚Üí Add to loss
                      ‚Üë
                   FAILS for subclassed models
```

### New Approach: Callback-Based Monitoring
```
Training Loop ‚Üí Keras Callback ‚Üí Monitor model output ‚Üí Log metrics
                                  ‚Üì
                              Compute redundancy score
                                  ‚Üì
                              Save to TensorBoard/CSV
```

### Why This Works
1. ‚úÖ **No layer introspection needed** - monitors public model output
2. ‚úÖ **Works with any model architecture** - functional or subclassed
3. ‚úÖ **Non-invasive** - doesn't modify training loop
4. ‚úÖ **Simple** - ~100-150 lines of clean code
5. ‚úÖ **Flexible** - easy to extend with custom metrics

---

## Complete Implementation Plan

### Phase 1: Fix Critical Architecture Issues

#### Step 1.1: Remove Skip Connections from Nano_U
**File**: [`src/models/Nano_U/model_tf.py`](../src/models/Nano_U/model_tf.py)  
**Lines**: 84-110

**Change**:
```python
# REMOVE this code:
def call(self, inputs, training=False):
    x = inputs
    skips = []
    
    # Encoder - REMOVE skip saving
    for i, conv in enumerate(self.enc_convs):
        x = conv(x, training=training)
        if i < self.num_levels - 1:
            skips.append(x)  # ‚Üê DELETE THIS
            x = self.enc_pools[i](x)

    x = self.bottleneck(x, training=training)

    # Decoder - REMOVE skip connections
    for up, conv in zip(self.dec_ups, self.dec_convs):
        x = up(x)
        if skips:  # ‚Üê DELETE THIS ENTIRE BLOCK
            skip = skips.pop()
            if tf.shape(x)[1] != tf.shape(skip)[1] or tf.shape(x)[2] != tf.shape(skip)[2]:
                skip = tf.image.resize(skip, size=(tf.shape(x)[1], tf.shape(x)[2]), method='bilinear')
            x = layers.Concatenate(name=f'skip_concat_{len(skips)}')([x, skip])
        x = conv(x, training=training)

    logits = self.output_conv(x)
    return logits

# REPLACE with:
def call(self, inputs, training=False):
    x = inputs
    
    # Encoder - simple forward pass
    for i, conv in enumerate(self.enc_convs):
        x = conv(x, training=training)
        if i < self.num_levels - 1:
            x = self.enc_pools[i](x)

    # Bottleneck
    x = self.bottleneck(x, training=training)

    # Decoder - simple upsampling
    for up, conv in zip(self.dec_ups, self.dec_convs):
        x = up(x)
        x = conv(x, training=training)

    logits = self.output_conv(x)
    return logits
```

**Test**: Run architecture validation script

---

### Phase 2: Implement NAS Monitoring Callback

#### Step 2.1: Create NASMonitorCallback Class
**File**: [`src/nas_covariance.py`](../src/nas_covariance.py)  
**Location**: Add after existing classes (~line 583)

**Implementation**:
```python
class NASMonitorCallback(tf.keras.callbacks.Callback):
    """
    Non-invasive NAS monitoring via Keras callback.
    
    Monitors feature redundancy during training by computing covariance
    statistics on model outputs. Works with any model architecture.
    
    Args:
        monitor_frequency: 'batch' or 'epoch'
        log_frequency: Log every N batches/epochs
        redundancy_weight: Optional - add redundancy to loss if > 0
        log_dir: Directory for TensorBoard logs
        save_history: Save metrics to CSV
    """
    
    def __init__(self, 
                 monitor_frequency='epoch',
                 log_frequency=1,
                 redundancy_weight=0.0,
                 log_dir=None,
                 save_history=True):
        super().__init__()
        self.monitor_frequency = monitor_frequency
        self.log_frequency = log_frequency
        self.redundancy_weight = redundancy_weight
        self.log_dir = log_dir
        self.save_history = save_history
        
        # Metrics storage
        self.redundancy_history = []
        self.correlation_history = []
        self.epoch_history = []
        self.batch_count = 0
        
        # TensorBoard writer
        if log_dir:
            self.tb_writer = tf.summary.create_file_writer(log_dir)
        else:
            self.tb_writer = None
    
    def on_epoch_end(self, epoch, logs=None):
        """Monitor redundancy at end of each epoch."""
        if self.monitor_frequency != 'epoch':
            return
        
        if epoch % self.log_frequency != 0:
            return
        
        # Get validation data
        if not hasattr(self.model, 'validation_data') or self.model.validation_data is None:
            return
        
        # Compute metrics on validation batch
        val_data = self.model.validation_data
        if isinstance(val_data, tf.data.Dataset):
            # Take first batch
            for x_val, y_val in val_data.take(1):
                metrics = self._compute_metrics(x_val)
                self._log_metrics(epoch, metrics)
                break
    
    def on_batch_end(self, batch, logs=None):
        """Monitor redundancy at end of each batch (if enabled)."""
        if self.monitor_frequency != 'batch':
            return
        
        self.batch_count += 1
        if self.batch_count % self.log_frequency != 0:
            return
        
        # Note: batch-level monitoring requires access to batch data
        # This is more complex and optional for now
        pass
    
    def _compute_metrics(self, x):
        """Compute redundancy metrics on model output."""
        # Get model output
        y_pred = self.model(x, training=False)
        
        # Compute covariance-based redundancy
        redundancy_score, metrics_dict = covariance_redundancy(
            y_pred, 
            normalize=True, 
            return_metrics=True
        )
        
        return {
            'redundancy_score': float(redundancy_score.numpy()),
            'trace': float(metrics_dict.get('trace', 0)),
            'mean_correlation': float(metrics_dict.get('mean_correlation', 0)),
            'condition_number': float(metrics_dict.get('condition_number', 0))
        }
    
    def _log_metrics(self, step, metrics):
        """Log metrics to console, TensorBoard, and history."""
        # Console
        print(f"\nNAS Metrics (step {step}):")
        for key, value in metrics.items():
            print(f"  {key}: {value:.6f}")
        
        # TensorBoard
        if self.tb_writer:
            with self.tb_writer.as_default():
                for key, value in metrics.items():
                    tf.summary.scalar(f'nas/{key}', value, step=step)
                self.tb_writer.flush()
        
        # History
        if self.save_history:
            self.redundancy_history.append(metrics['redundancy_score'])
            self.correlation_history.append(metrics.get('mean_correlation', 0))
            self.epoch_history.append(step)
    
    def get_metrics(self):
        """Return collected metrics."""
        return {
            'epochs': self.epoch_history,
            'redundancy': self.redundancy_history,
            'correlation': self.correlation_history
        }
    
    def save_metrics_csv(self, filepath='nas_metrics.csv'):
        """Save metrics to CSV file."""
        import pandas as pd
        df = pd.DataFrame({
            'epoch': self.epoch_history,
            'redundancy_score': self.redundancy_history,
            'mean_correlation': self.correlation_history
        })
        df.to_csv(filepath, index=False)
        print(f"NAS metrics saved to {filepath}")
```

**Test**: Create unit test for callback

---

#### Step 2.2: Integrate Callback into train.py
**File**: [`src/train.py`](../src/train.py)  
**Location**: Modify train() function

**Changes**:
```python
def train(model_name="nano_u", epochs=None, batch_size=None, lr=None,
          distill=False, teacher_weights=None, alpha=None, temperature=None,
          augment=True, config_path="config/config.yaml",
          enable_nas_monitoring=False, nas_config=None):  # ‚Üê ADD THESE PARAMS
    
    # ... existing code ...
    
    # Setup callbacks
    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(...),
        tf.keras.callbacks.ReduceLROnPlateau(...),
        tf.keras.callbacks.EarlyStopping(...)
    ]
    
    # Add NAS monitoring callback if requested
    if enable_nas_monitoring:
        from src.nas_covariance import NASMonitorCallback
        
        nas_config = nas_config or {}
        nas_callback = NASMonitorCallback(
            monitor_frequency=nas_config.get('monitor_frequency', 'epoch'),
            log_frequency=nas_config.get('log_frequency', 1),
            redundancy_weight=nas_config.get('redundancy_weight', 0.0),
            log_dir=nas_config.get('log_dir', 'logs/nas'),
            save_history=True
        )
        callbacks.append(nas_callback)
        print(f"‚úì NAS monitoring enabled (frequency: {nas_config.get('monitor_frequency', 'epoch')})")
    
    # ... rest of training ...
```

---

#### Step 2.3: Simplify train_with_nas.py
**File**: [`src/train_with_nas.py`](../src/train_with_nas.py)

**Strategy**: Simplify by delegating to train.py with NAS enabled

**Option A - Complete Replacement** (Recommended):
```python
"""
NAS-enabled training - simplified to use callback approach.
This file now delegates to src/train.py with enable_nas_monitoring=True.
"""
import sys
from src.train import train

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="config/config.yaml")
    parser.add_argument("--model", default="nano_u", choices=["nano_u", "bu_net"])
    parser.add_argument("--epochs", type=int, default=None)
    parser.add_argument("--batch-size", type=int, default=None)
    parser.add_argument("--lr", type=float, default=None)
    parser.add_argument("--distill", action="store_true")
    parser.add_argument("--teacher-weights", type=str, default=None)
    parser.add_argument("--alpha", type=float, default=None)
    parser.add_argument("--temperature", type=float, default=None)
    parser.add_argument("--no-augment", action="store_true")
    parser.add_argument("--enable-nas", action="store_true", default=True)  # Default ON
    parser.add_argument("--nas-frequency", type=str, default="epoch")
    parser.add_argument("--nas-log-freq", type=int, default=1)
    args = parser.parse_args()
    
    nas_config = {
        'monitor_frequency': args.nas_frequency,
        'log_frequency': args.nas_log_freq,
        'log_dir': 'logs/nas',
        'save_history': True
    }
    
    train(
        model_name=args.model,
        epochs=args.epochs,
        batch_size=args.batch_size,
        lr=args.lr,
        distill=args.distill,
        teacher_weights=args.teacher_weights,
        alpha=args.alpha,
        temperature=args.temperature,
        augment=not args.no_augment,
        config_path=args.config,
        enable_nas_monitoring=args.enable_nas,
        nas_config=nas_config
    )
```

**Option B - Keep Complex Implementation** (Not Recommended):
- Remove NASWrapper and DistillerWithNAS classes (lines 49-206)
- Remove ActivationExtractor usage (lines 348-410)
- Replace with simple callback instantiation
- Much more work, same result

**Decision**: Use Option A - complete replacement

---

### Phase 3: Visualization & Metrics Plotting

#### Step 3.1: Create NAS Visualization Script
**File**: `src/plot_nas_metrics.py` (NEW)

**Purpose**: Generate comprehensive plots to analyze NAS metrics and guide architecture decisions

**Implementation**:
```python
"""
Plot NAS metrics for model architecture analysis.

This script visualizes redundancy trends during training to help identify:
- Whether the model has redundant features
- If filter sizes should be reduced
- Optimal bottleneck size
- Training dynamics
"""

import argparse
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path

def plot_nas_metrics(csv_path, output_dir='results/nas_plots'):
    """
    Create comprehensive NAS metric visualizations.
    
    Args:
        csv_path: Path to CSV file with NAS metrics
        output_dir: Directory to save plots
    """
    # Load data
    df = pd.read_csv(csv_path)
    
    # Create output directory
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # ========== Plot 1: Redundancy Over Training ==========
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.plot(df['epoch'], df['redundancy_score'], 
            linewidth=2, color='#2E86AB', marker='o', markersize=4)
    ax.axhline(y=df['redundancy_score'].mean(), 
               color='r', linestyle='--', alpha=0.7, 
               label=f'Mean: {df["redundancy_score"].mean():.4f}')
    ax.fill_between(df['epoch'], 
                     df['redundancy_score'].min(), 
                     df['redundancy_score'].max(), 
                     alpha=0.2, color='#2E86AB')
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Redundancy Score', fontsize=12)
    ax.set_title('Feature Redundancy During Training', fontsize=14, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'{output_dir}/redundancy_over_time.png', dpi=300)
    print(f'‚úì Saved redundancy plot: {output_dir}/redundancy_over_time.png')
    plt.close()
    
    # ========== Plot 2: Correlation Analysis ==========
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.plot(df['epoch'], df['mean_correlation'], 
            linewidth=2, color='#A23B72', marker='s', markersize=4)
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Mean Feature Correlation', fontsize=12)
    ax.set_title('Feature Correlation During Training', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # Add interpretation zones
    ax.axhspan(0, 0.3, alpha=0.1, color='green', label='Low correlation (good)')
    ax.axhspan(0.3, 0.6, alpha=0.1, color='yellow', label='Medium correlation')
    ax.axhspan(0.6, 1.0, alpha=0.1, color='red', label='High correlation (redundant)')
    ax.legend(loc='upper right')
    plt.tight_layout()
    plt.savefig(f'{output_dir}/correlation_analysis.png', dpi=300)
    print(f'‚úì Saved correlation plot: {output_dir}/correlation_analysis.png')
    plt.close()
    
    # ========== Plot 3: Multi-Metric Dashboard ==========
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('NAS Analysis Dashboard', fontsize=16, fontweight='bold')
    
    # Subplot 1: Redundancy
    axes[0, 0].plot(df['epoch'], df['redundancy_score'], color='#2E86AB', linewidth=2)
    axes[0, 0].set_title('Redundancy Score')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].grid(True, alpha=0.3)
    
    # Subplot 2: Correlation
    axes[0, 1].plot(df['epoch'], df['mean_correlation'], color='#A23B72', linewidth=2)
    axes[0, 1].set_title('Mean Correlation')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].grid(True, alpha=0.3)
    
    # Subplot 3: Redundancy Distribution
    axes[1, 0].hist(df['redundancy_score'], bins=30, color='#2E86AB', alpha=0.7, edgecolor='black')
    axes[1, 0].axvline(df['redundancy_score'].mean(), color='r', linestyle='--', linewidth=2, label='Mean')
    axes[1, 0].set_title('Redundancy Distribution')
    axes[1, 0].set_xlabel('Redundancy Score')
    axes[1, 0].set_ylabel('Frequency')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # Subplot 4: Summary Statistics Table
    axes[1, 1].axis('off')
    summary_data = [
        ['Metric', 'Value'],
        ['Mean Redundancy', f'{df["redundancy_score"].mean():.4f}'],
        ['Std Redundancy', f'{df["redundancy_score"].std():.4f}'],
        ['Final Redundancy', f'{df["redundancy_score"].iloc[-1]:.4f}'],
        ['Mean Correlation', f'{df["mean_correlation"].mean():.4f}'],
        ['Max Correlation', f'{df["mean_correlation"].max():.4f}'],
        ['Training Epochs', f'{len(df)}']
    ]
    table = axes[1, 1].table(cellText=summary_data, cellLoc='left', 
                             loc='center', colWidths=[0.4, 0.3])
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)
    
    # Style header row
    for i in range(2):
        table[(0, i)].set_facecolor('#2E86AB')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}/nas_dashboard.png', dpi=300)
    print(f'‚úì Saved dashboard: {output_dir}/nas_dashboard.png')
    plt.close()
    
    # ========== Generate Recommendations ==========
    mean_redundancy = df['redundancy_score'].mean()
    final_redundancy = df['redundancy_score'].iloc[-1]
    mean_correlation = df['mean_correlation'].mean()
    
    print("\n" + "="*60)
    print("NAS ANALYSIS RECOMMENDATIONS")
    print("="*60)
    
    print(f"\nüìä Redundancy Score: {mean_redundancy:.4f}")
    if mean_redundancy > 0.5:
        print("  ‚ö†Ô∏è  HIGH REDUNDANCY DETECTED")
        print("  ‚Üí Consider reducing filter sizes")
        print("  ‚Üí Consider reducing bottleneck size")
        print("  ‚Üí Model may be overparameterized")
    elif mean_redundancy > 0.3:
        print("  ‚öôÔ∏è  MODERATE REDUNDANCY")
        print("  ‚Üí Model size is reasonable")
        print("  ‚Üí Could potentially reduce by 10-20%")
    else:
        print("  ‚úÖ LOW REDUNDANCY")
        print("  ‚Üí Model is well-sized")
        print("  ‚Üí Features are diverse")
    
    print(f"\nüìä Mean Correlation: {mean_correlation:.4f}")
    if mean_correlation > 0.6:
        print("  ‚ö†Ô∏è  FEATURES ARE HIGHLY CORRELATED")
        print("  ‚Üí Reduce number of filters")
    elif mean_correlation > 0.3:
        print("  ‚öôÔ∏è  MODERATE CORRELATION")
        print("  ‚Üí Architecture is reasonable")
    else:
        print("  ‚úÖ FEATURES ARE DIVERSE")
        print("  ‚Üí Good feature learning")
    
    print(f"\nüìä Redundancy Trend:")
    if final_redundancy < mean_redundancy * 0.8:
        print("  ‚úÖ DECREASING - Model is learning to use features efficiently")
    elif final_redundancy > mean_redundancy * 1.2:
        print("  ‚ö†Ô∏è  INCREASING - Model may be overfitting or has too many filters")
    else:
        print("  ‚öôÔ∏è  STABLE - Consistent feature usage")
    
    # Specific architecture recommendations
    print("\nüìã Architecture Recommendations:")
    if mean_redundancy > 0.5:
        print("  ‚Ä¢ Reduce encoder filters by 25-30%")
        print("  ‚Ä¢ Reduce bottleneck size by 25%")
        print("  ‚Ä¢ Consider using smaller conv blocks")
    elif mean_redundancy > 0.3:
        print("  ‚Ä¢ Reduce encoder filters by 10-15%")
        print("  ‚Ä¢ Keep bottleneck size")
    else:
        print("  ‚Ä¢ Current architecture is optimal")
        print("  ‚Ä¢ Consider increasing capacity if underfitting")
    
    print("\n" + "="*60)

def compare_models(csv_paths, model_names, output_dir='results/nas_plots'):
    """
    Compare NAS metrics across multiple models.
    
    Args:
        csv_paths: List of paths to CSV files
        model_names: List of model names
        output_dir: Directory to save plots
    """
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    fig.suptitle('Model Comparison - NAS Metrics', fontsize=16, fontweight='bold')
    
    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']
    
    for i, (csv_path, name) in enumerate(zip(csv_paths, model_names)):
        df = pd.read_csv(csv_path)
        color = colors[i % len(colors)]
        
        # Plot redundancy
        axes[0].plot(df['epoch'], df['redundancy_score'], 
                    linewidth=2, color=color, marker='o', markersize=3, label=name)
        
        # Plot correlation
        axes[1].plot(df['epoch'], df['mean_correlation'], 
                    linewidth=2, color=color, marker='s', markersize=3, label=name)
    
    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].set_ylabel('Redundancy Score', fontsize=12)
    axes[0].set_title('Redundancy Comparison')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    axes[1].set_xlabel('Epoch', fontsize=12)
    axes[1].set_ylabel('Mean Correlation', fontsize=12)
    axes[1].set_title('Correlation Comparison')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}/model_comparison.png', dpi=300)
    print(f'‚úì Saved comparison plot: {output_dir}/model_comparison.png')
    plt.close()

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Plot NAS metrics for architecture analysis')
    parser.add_argument('--csv', type=str, required=True, help='Path to NAS metrics CSV file')
    parser.add_argument('--output-dir', type=str, default='results/nas_plots', 
                       help='Output directory for plots')
    parser.add_argument('--compare', nargs='+', type=str, 
                       help='Compare multiple CSV files (space-separated)')
    parser.add_argument('--model-names', nargs='+', type=str,
                       help='Model names for comparison (space-separated)')
    
    args = parser.parse_args()
    
    if args.compare:
        if not args.model_names or len(args.model_names) != len(args.compare):
            print("Error: --model-names must have same length as --compare")
            exit(1)
        compare_models(args.compare, args.model_names, args.output_dir)
    else:
        plot_nas_metrics(args.csv, args.output_dir)
```

**Usage Examples**:
```bash
# Plot metrics from single training run
python src/plot_nas_metrics.py --csv nas_metrics.csv

# Compare multiple models
python src/plot_nas_metrics.py --compare \
    nas_nano_u.csv nas_bu_net.csv \
    --model-names "Nano_U" "BU_Net"
```

---

### Phase 4: Configuration & Documentation

#### Step 4.1: Update config.yaml
**File**: [`config/config.yaml`](../config/config.yaml)

**Add NAS section**:
```yaml
nas:
  enabled: false  # Default disabled for regular training
  monitor_frequency: epoch  # or 'batch'
  log_frequency: 1  # Log every N epochs/batches
  redundancy_weight: 0.0  # Optional: add to loss (0.0 = monitoring only)
  log_dir: logs/nas  # TensorBoard directory
  save_csv: true  # Save metrics to CSV
  csv_path: nas_metrics.csv
```

#### Step 4.2: Update docs/NAS_README.md
**File**: [`docs/NAS_README.md`](../docs/NAS_README.md)

**Update content to reflect new callback approach** - document:
- How NASMonitorCallback works
- When to use NAS monitoring
- How to interpret metrics
- Architecture decisions based on redundancy scores
- Usage examples

---

### Phase 5: Testing & Validation

#### Test 5.1: Architecture Validation
**Script**: `tests/test_architecture_fixes.py` (already exists)

**Run**:
```bash
pytest tests/test_architecture_fixes.py -v
```

**Expected**: All tests pass, Nano_U has no skip connections

#### Test 5.2: Basic Training (No NAS)
```bash
python src/train.py --model nano_u --epochs 2 --batch-size 4
```

**Expected**: Training completes without errors

#### Test 5.3: NAS Monitoring Training
```bash
python src/train_with_nas.py --model nano_u --epochs 5 --batch-size 4 --enable-nas
```

**Expected**: 
- Training completes
- NAS metrics logged every epoch
- nas_metrics.csv created
- No errors about layer detection

#### Test 5.4: Visualization
```bash
python src/plot_nas_metrics.py --csv nas_metrics.csv --output-dir results/nas_plots
```

**Expected**:
- 3 PNG files created in results/nas_plots/
- Recommendations printed to console
- No errors

#### Test 5.5: Model Comparison
Train both models with NAS, then compare:
```bash
# Train Nano_U
python src/train_with_nas.py --model nano_u --epochs 10 --enable-nas
mv nas_metrics.csv nas_nano_u.csv

# Train BU_Net
python src/train_with_nas.py --model bu_net --epochs 10 --enable-nas  
mv nas_metrics.csv nas_bu_net.csv

# Compare
python src/plot_nas_metrics.py --compare nas_nano_u.csv nas_bu_net.csv \
    --model-names "Nano_U" "BU_Net"
```

**Expected**: Comparison plot shows both models

---

## Visualization & Metrics

### Plots Generated

#### 1. Redundancy Over Time
- X-axis: Training epochs
- Y-axis: Redundancy score (0-1)
- Shows: How redundancy changes during training
- Interpretation:
  - Decreasing = model learning to use features efficiently
  - Increasing = possible overfitting or too many filters
  - Stable = consistent feature usage

#### 2. Correlation Analysis
- X-axis: Training epochs
- Y-axis: Mean feature correlation (0-1)
- Color zones:
  - Green (0-0.3): Low correlation, good diversity
  - Yellow (0.3-0.6): Moderate correlation
  - Red (0.6-1.0): High correlation, redundant features

#### 3. NAS Dashboard
4-panel view:
- Top-left: Redundancy trend
- Top-right: Correlation trend
- Bottom-left: Redundancy distribution histogram
- Bottom-right: Summary statistics table

#### 4. Model Comparison
Side-by-side plots comparing redundancy and correlation across multiple models

### Interpretation Guide

| Redundancy Score | Meaning | Action |
|-----------------|---------|--------|
| > 0.5 | High redundancy | Reduce filters by 25-30% |
| 0.3 - 0.5 | Moderate redundancy | Reduce filters by 10-15% |
| < 0.3 | Low redundancy | Architecture is optimal |

| Correlation | Meaning | Action |
|-------------|---------|--------|
| > 0.6 | Highly correlated features | Reduce filter count |
| 0.3 - 0.6 | Moderately correlated | Architecture OK |
| < 0.3 | Diverse features | Good feature learning |

---

## Success Criteria

### Functional Requirements ‚úÖ
- [ ] Nano_U has no skip connections
- [ ] Basic training works without NAS
- [ ] NAS monitoring training works
- [ ] Metrics are logged correctly
- [ ] Plots are generated successfully
- [ ] No layer detection errors

### Performance Requirements ‚úÖ
- [ ] Training overhead < 5% with NAS monitoring
- [ ] Memory usage doesn't spike
- [ ] Plots generate in < 10 seconds

### Code Quality ‚úÖ
- [ ] No complex workarounds (inner_model removed)
- [ ] Clean, readable code
- [ ] Proper error handling
- [ ] Comprehensive documentation

### Usability ‚úÖ
- [ ] Single flag to enable NAS: `--enable-nas`
- [ ] Clear console output
- [ ] Easy to interpret plots
- [ ] Actionable recommendations

---

## Final Checklist

### Phase 1: Architecture ‚úÖ COMPLETED
- [x] Remove skip connections from Nano_U
- [x] Test model builds correctly
- [x] Verify parameter count

### Phase 2: NAS Callback ‚úÖ COMPLETED
- [x] Create NASMonitorCallback class
- [x] Integrate into train.py
- [x] Simplify train_with_nas.py
- [ ] Test callback works

### Phase 3: Visualization ‚úÖ COMPLETED
- [x] Create plot_nas_metrics.py
- [x] Implement all 4 plot types
- [x] Add recommendation logic
- [ ] Test with sample data

### Phase 4: Documentation üîÑ IN PROGRESS
- [x] Update config.yaml
- [ ] Update docs/NAS_README.md
- [ ] Add usage examples
- [ ] Document interpretation

### Phase 5: Testing ‚è≥ PENDING
- [ ] Run architecture tests
- [ ] Test basic training
- [ ] Test NAS training
- [ ] Test visualization
- [ ] Test model comparison

---

## Commands Quick Reference

```bash
# 1. Test architecture
pytest tests/test_architecture_fixes.py -v

# 2. Train without NAS (baseline)
python src/train.py --model nano_u --epochs 10

# 3. Train with NAS monitoring
python src/train_with_nas.py --model nano_u --epochs 10 --enable-nas

# 4. Generate plots
python src/plot_nas_metrics.py --csv nas_metrics.csv

# 5. Compare models
python src/plot_nas_metrics.py --compare nas_nano_u.csv nas_bu_net.csv \
    --model-names "Nano_U" "BU_Net"

# 6. Full pipeline test
./scripts/tf_pipeline.sh
```

---

## Expected Timeline

### Implementation
- Phase 1 (Architecture): Remove skip connections ‚Üí Test ‚Üí DONE
- Phase 2 (NAS Callback): Implement callback ‚Üí Integrate ‚Üí Test
- Phase 3 (Visualization): Create plots ‚Üí Test ‚Üí DONE
- Phase 4 (Documentation): Update docs ‚Üí DONE
- Phase 5 (Testing): Full validation ‚Üí DONE

### Testing
- Unit tests per phase
- Integration test at end
- Manual verification of plots

---

**Status**: üìã Planning Complete - Ready to Switch to Code Mode

**Next Action**: Switch to Code mode to implement Phase 1 (Architecture fixes)
